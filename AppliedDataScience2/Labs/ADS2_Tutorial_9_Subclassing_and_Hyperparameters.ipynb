{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ADS2 - Tutorial 8 - Subclassing and Hyperparameters.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial 8 - Subclassing and Hyperparameters\n",
        "\n",
        "Today you will be creating a simple \"Multi-layer Perceptron\" model by subclassing the keras Model class. You wil then test that model with a variety of different settings, to explore how changing hyperparameters affects model training."
      ],
      "metadata": {
        "id": "JI5fw26KIq2H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21eKLH2r7sHp"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 1\n",
        "\n",
        "The outline of a model class is provided below. The model is a multi-layer perceptron with four dense layers, which will predict class probabilities . In the `__init__` method, you need to create the individual layers and assign them to the model's `self`. In the `call()` method, you should pass the input through each of layers, and then return the output of the final layer."
      ],
      "metadata": {
        "id": "Mptwc2Y5KOre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP_Model(models.Model):\n",
        "    def __init__(self, units, activations, out_units, out_activation):\n",
        "        \"\"\"\n",
        "        A Multi-layer perceptron model, subclassing models.Model.\n",
        "\n",
        "        args:\n",
        "            units, list of integers giving number of units in first three layers\n",
        "            activations, list of strings giving the activations of the first\n",
        "                         three layers\n",
        "            out_units, number of units in the output layer\n",
        "            out_activation, activation function of the output layer\n",
        "        \"\"\"\n",
        "        super(MLP_Model, self).__init__()\n",
        "\n",
        "        ### Set up the dense layers and assign them to self\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        # call method takes in inputs and passes it through each of the layers\n",
        "        # in succession\n",
        "\n",
        "        return "
      ],
      "metadata": {
        "id": "2LFchZ5y-zyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 2\n",
        "\n",
        "Call the MLP_Model class with the following settings:\n",
        "\n",
        "```\n",
        "units = [512, 256, 128]\n",
        "activations = ['relu', 'relu', 'relu']\n",
        "out_units = 10\n",
        "out_activation = 'softmax'\n",
        "```\n",
        "\n",
        "Compile the model for training on the MNIST image dataset, then load and prepare the dataset for training.\n",
        "\n",
        "Save the model weights so you can reload the initial settings later."
      ],
      "metadata": {
        "id": "bOo3Fwq4LSb3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Create an instance of the MLP_Model with the stated settings.\n",
        "### Save the initial model weights so we can reset the model to the same\n",
        "### initial weights.\n"
      ],
      "metadata": {
        "id": "-U2SvgDkGCcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Compile the model with the Adam optimizer, Sparse Categorical Crossentropy\n",
        "### and the accuracy metric\n"
      ],
      "metadata": {
        "id": "_Ah41klTZlr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the MNIST image dataset, flatten the images and rescale the pixels\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train = x_train.reshape(-1, 784)/255.\n",
        "x_test = x_test.reshape(-1, 784)/255."
      ],
      "metadata": {
        "id": "bb87kjSkG27W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 3\n",
        "\n",
        "Train the model using the fit method, and include the test data for validation. A batch size of 256 trains quite quickly on this model. When you call the fit method, you can store a dictionary of losses from the training like so:\n",
        "\n",
        "```\n",
        "history = model.fit(...)\n",
        "history.history # a dictionary of the losses and metrics at each epoch\n",
        "# check the values available with:\n",
        "history.history.keys()\n",
        "```\n",
        "\n",
        "Make a plots of the losses and metrics for the training and test data."
      ],
      "metadata": {
        "id": "-cBUYhckLumv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Train the model for 20 epochs, include the test data for validation\n",
        "### Store the losses and metrics as detailed above\n"
      ],
      "metadata": {
        "id": "VURn4MZHHRin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Plot the losses and metrics for the training and test data\n"
      ],
      "metadata": {
        "id": "YyxEPXrcL4bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 4\n",
        "\n",
        "Up until now we have used the default settings for the Adam optimiser, with a learning rate of 0.001. Test a range of new learning rates on the model by loading the initial weights, then recompiling the model with the Adam optimiser. Train for the same number of epochs each time, and then produce a plot that compares the evolution of the loss function for each value of the learning rate.\n",
        "\n",
        "Learning rates to test:\n",
        "```\n",
        "0.01, 1e-4, 1e-5\n",
        "```"
      ],
      "metadata": {
        "id": "DY2-rdwHN_Nh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Recreate the MLP_Model and compile it with a range of learning rate values\n"
      ],
      "metadata": {
        "id": "zK9qLsgeRdDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Recreate the MLP_Model and compile it with a range of learning rate values\n"
      ],
      "metadata": {
        "id": "1dPd_iM-Rdcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Recreate the MLP_Model and compile it with a range of learning rate values\n"
      ],
      "metadata": {
        "id": "BVGpxXNYReBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Create a plot that compares the losses and accuracies of the MLP_Model\n",
        "### trained with different learning rates.\n",
        "\n"
      ],
      "metadata": {
        "id": "5vaqWMo4RqRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 5\n",
        "\n",
        "Experiment with changing any of the settings of the modelâ€”with the exception of the number of units and the activation of the final Dense layer. Try changing the number of units in the first three Dense layers, or their [activation functions](https://www.tensorflow.org/api_docs/python/tf/keras/activations). Try a different [optimiser algorithm](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)."
      ],
      "metadata": {
        "id": "NS6yX2OMP5bk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BO6Ujc6Fm12G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}