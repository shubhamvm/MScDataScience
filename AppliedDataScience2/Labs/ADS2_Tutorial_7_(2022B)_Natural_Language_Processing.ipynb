{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial 5 - RNNs\n",
        "\n",
        "In this tutorial, you will build a recurrent neural network (RNN), with long-short term memory (LSTM) units, in order to classify movie reviews from the IMDB dataset. As discussed in the lectures, RNNs operate by holding a state vector that is a combination of all previous inputs in a sequence. LSTMs are a modification of standard RNN cells, which perform additional operations to filter out unimportant information, allowing for longer sequences to be learned.\n",
        "\n",
        "This tutorial has been adapted from the TensorFlow guides. https://www.tensorflow.org/text/tutorials/text_classification_rnn"
      ],
      "metadata": {
        "id": "0JFkNhFTqDKn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uavfSGDyctze"
      },
      "outputs": [],
      "source": [
        "# Module Imports\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "tfds.disable_progress_bar()\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 1 - Setting up the data\n",
        "\n",
        "As seen in the lectures, computers cannot handle text data. Therefore, it is necessary to convert the text to numbers. Here, we will use a vectorisation layer, which converts each word in a sentence to a unique integer.\n",
        "\n",
        "First, load in the dataset using the code provided. You can inspect the data by using the `train_dataset.take(1)` method, which will return a batch of (examples, labels). Print some of the texts and their labels.\n",
        "\n",
        "Next, set up the `TextVectorization` layer as described below. Use the `encoder.adapt()` method to generate the vocabulary, or corpus, of words that the model will understand. Print the top 20 words in the dataset, which you can access via the `encoder.get_vocabulary()` method.\n",
        "\n",
        "Pass a few examples of the data into the encoder and print the results. We can reverse the process by indexing the vocab with the encoded vectors. The decoded vectors are not exactly the same as the original texts, why?"
      ],
      "metadata": {
        "id": "3JSbk1ZHr0UC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to load the dataset from the tensorflow_datasets module\n",
        "# A tf.data iterator is set up for training and testing data (we will see these\n",
        "# in later lectures.)\n",
        "dataset, info = tfds.load('imdb_reviews', with_info=True,\n",
        "                          as_supervised=True)\n",
        "train_dataset, test_dataset = dataset['train'], dataset['test']\n",
        "\n",
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 64\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "Train_TextOnly = train_dataset.map(lambda text, label: text)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "T9BUtf6tg3MI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Print a few examples of the training dataset, along with their labels.\n",
        "# train_dataset.take(1)\n",
        "\n"
      ],
      "metadata": {
        "id": "8tULu6kIhALy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Set up a TextVectorzation layer, with a vocab size (max_tokens) of 1000.\n",
        "### Build the vocab of the model by calling the encoder.adapt() method on the\n",
        "### Train_TextOnly dataset. \n",
        "# tf.layers.TextVectorization, encoder.adapt\n",
        "\n",
        "VOCAB_SIZE = 1000\n",
        "encoder = \n"
      ],
      "metadata": {
        "id": "od8LicQghDu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Extract the vocabulary from the encoder, print the top 20 words.\n",
        "# encoder.get_vocabulary()\n",
        "\n",
        "# vocab should be a np.array\n",
        "vocab = \n"
      ],
      "metadata": {
        "id": "W1ul_7k2hFb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Encode a few example reviews and print the results.\n",
        "# encoder().numpy()\n",
        "\n",
        "encoded_example = "
      ],
      "metadata": {
        "id": "i4hbvZ1AhHLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Decode some example texts, by using the encoded vectors as indices for the\n",
        "### vocabulary.\n",
        "\n"
      ],
      "metadata": {
        "id": "tDPk8D1lhJBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 2 - Model set up\n",
        "\n",
        "We will construct our network as a keras sequential model.\n",
        "\n",
        "First, set up the embedding layer, which will take the encoded words and learn a suitable representation of the data.\n",
        "\n",
        "Next, build the sequential model with the following layers:\n",
        "```\n",
        "encoder\n",
        "embedder\n",
        "LSTM - 64 units\n",
        "Dense - 64 units, relu activation\n",
        "Dense - 1 unit, sigmoid activation\n",
        "```\n",
        "\n",
        "Compile the model with `BinaryCrossentropy` loss, and the Adam optimizer. Train the model for 10 epochs, storing the losses and metrics in a history object."
      ],
      "metadata": {
        "id": "HoQBdky1wqkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Set up the embedding layer. The input_dim is the length of the vocab, and\n",
        "### the output_dim is 64. You should also set mask_zero=True.\n",
        "# tf.keras.layers.Embedding\n",
        "\n",
        "embedder = "
      ],
      "metadata": {
        "id": "uyVZLhbyx6Ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Build a keras seuqential model, with the layers provided above.\n",
        "# tf.keras.layers.LSTM, tf.keras.layers.Dense\n",
        "model = tf.keras.Sequential([\n",
        "\n",
        "])"
      ],
      "metadata": {
        "id": "rvYA33dFhQ_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Compile the model with binary crossentropy loss, the adam optimizer, and \n",
        "### the accuracy metric\n",
        "# tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "# tf.keras.optimizers.Adam(1e-4)\n",
        "\n",
        "model.compile()"
      ],
      "metadata": {
        "id": "Pm-uK-Swhc07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model for 10 epochs.\n",
        "history = model.fit(train_dataset, epochs=10,\n",
        "                    validation_data=test_dataset,\n",
        "                    validation_steps=30)"
      ],
      "metadata": {
        "id": "JfntQwogheaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 3 - Evaluate the model\n",
        "\n",
        "Now that the model is trained, we can analyse its performance. Consider, during this exercise, not just what the numbers are, but the meaning behind them.\n",
        "\n",
        "Start by calculating the test loss and test accuracy, using the `model.evaluate()` method. Print these values.\n",
        "\n",
        "Create two plots, one for the loss and one for the accuracy, containing the values for the training and testing data from the history object. How do the test and train values compare? How do they evolve overtime? Why?\n",
        "\n",
        "Extract a few examples from the test dataset, and use the `model.predict()` method to classify them as positive or negative. Do you agree with the model? How do the predicted labels compare to the true labels? Can you tell why it might be getting some examples wrong?\n",
        "\n",
        "Lastly, write a short review of the last movie you watched, and use the model to predict whether it is positive or negative. Is it right?"
      ],
      "metadata": {
        "id": "Ym45cqeizHSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Use the model.evaluate method to calculate the test loss and accuracy\n",
        "# model.evaluate()\n",
        "\n",
        "test_loss, test_acc = \n"
      ],
      "metadata": {
        "id": "RjLw4AJshgwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Create plots of the training and test losses and metrics\n",
        "# history.history"
      ],
      "metadata": {
        "id": "3vjC52tG0kJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Extract a batch of examples, and pass the texts in the model.predict method\n",
        "### to classify them as positive (close to 1) or negative (close to 0)\n",
        "# model.predict\n",
        "\n",
        "examples = [(texts, labels) for texts, labels in test_dataset.take(1)]\n",
        "\n",
        "texts = examples[0][0]\n",
        "labels = examples[0][1]\n"
      ],
      "metadata": {
        "id": "_Kq_c1sZ0n8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Write a short review of the last movie you watched and use the model to\n",
        "### predict if it is positive or negative\n",
        "\n",
        "sample_review = np.array([('Text of your review')])\n"
      ],
      "metadata": {
        "id": "5xAypGJH1eh1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}