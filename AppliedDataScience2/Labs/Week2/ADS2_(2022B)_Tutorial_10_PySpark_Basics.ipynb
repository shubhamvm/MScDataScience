{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViQdeQ5m1E2Z"
      },
      "source": [
        "# ADS2 - Tutorial 2 - PySpark Basics\n",
        "Learning Outcomes:\n",
        "*   Importing data into Spark dataframes from .csv files\n",
        "*   Exploring and manipulating data tables with Spark SQL\n",
        "*   Write data to a file\n",
        "\n",
        "**Methods and Functions:**\n",
        "\n",
        "\n",
        "```\n",
        "spark\n",
        "    .read\n",
        "    .sql\n",
        "\n",
        "dataframe\n",
        "    .show()\n",
        "    .printSchema\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "To begin, colab doesn't come with PySpark available by default, you will need to run the filling blocks of code to install it.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "oMF6GzXgQ8KO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lb-Z7ZM8O3s"
      },
      "outputs": [],
      "source": [
        "# Apache Spark uses Java, so first we must install that\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixa72o938SKP"
      },
      "outputs": [],
      "source": [
        "# Unpack Spark from google drive\n",
        "!tar xzf /content/drive/MyDrive/spark-3.3.0-bin-hadoop3.tgz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWT7_XiQ8V6u"
      },
      "outputs": [],
      "source": [
        "# Set up environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"spark-3.3.0-bin-hadoop3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPSvq-aj8Z_k"
      },
      "outputs": [],
      "source": [
        "# Install findspark, which helps python locate the psyspark module files\n",
        "!pip install -q findspark\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXT8Q_IO8cVe"
      },
      "outputs": [],
      "source": [
        "# Finally, we initialse a \"SparkSession\", which handles the computations\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N61alIq17Q4i"
      },
      "source": [
        "# Exercise 1\n",
        "\n",
        "for this tutorial, you will explore a dataset of house price data from California. The .csv for this data is available on canvas, as well as from [Kaggle](https://www.kaggle.com/camnugent/california-housing-prices).\n",
        "\n",
        " * longitude\n",
        " * latitude\n",
        " * housing_median_age\n",
        " * total_rooms\n",
        " * total_bedrooms\n",
        " * population\n",
        " * households\n",
        " * median_income\n",
        " * median_house_value\n",
        " * ocean_proximity\n",
        "\n",
        "Download the data, then upload it in the files panel on the left of the colab window. You can copy the path to this file by right-clicking it after it's uploaded.\n",
        "\n",
        "Begin by loading the dataset into a Spark DataFrame. Certain options can to be set using the `.read.option(key, value)` methods. A full list of the options for .CSV files can be found here: [CSV Files](https://spark.apache.org/docs/latest/sql-data-sources-csv.html). Set the sperator option to commas, and the header option to True.\n",
        "\n",
        "Finally, you need to tell the Reader where the .csv file is.\n",
        "\n",
        "When the file is loaded, show the first 5 rows of the data and print the schema."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7PS8iZgqwjt"
      },
      "outputs": [],
      "source": [
        "### Load the California Housing Prices Dataset\n",
        "# .read, .option, .csv\n",
        "\n",
        "\n",
        "### Show the first 5 rows of data and print the schema\n",
        "# .show, .printSchema\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0B-nVXBnZK-"
      },
      "source": [
        "The Schema is the database blueprint which specifies the column name, data type, whether the field is nullable, and any extra metadata. In PySpark, these are StructTypes and StructFields. You should have found that the DataFrame you loaded has only strings as the datatypes. This isn't useful for numerical data. fortunately, there are a number of ways to set the schema of a DataFrame when you load it.\n",
        "\n",
        "The first, and simplest way, is to set the `inferSchema` option to `True` in the read call. Reload the DataFrame with this option set, and preint the schema."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQVtn8KcmQ69"
      },
      "outputs": [],
      "source": [
        "### Load the DataFrame again, this time with the inferSchema option enabled\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suolICfLo7rU"
      },
      "source": [
        "For very large tables, inferring the schema can be computationally costly, as PySpark must run an additional pass over the dataset. Instead, you can set predefined schema. One way to do this is by defining a StructType, with a list individual StructFields for each column.\n",
        "\n",
        "```\n",
        "schema = StructType([StructField_1, StructField_2, ...])\n",
        "```\n",
        "\n",
        "The first StructField is provided below, complete the list for all the columns in the dataset.\n",
        "\n",
        "Reload the DataFrame, this time replacing the inferSchema `.option()` call, with `.schema(userDefinedSchema)`. Print the new schema and check that it is correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXdVXii-p5lg"
      },
      "outputs": [],
      "source": [
        "### Load the DataFrame using a schema define with StructType and StructField\n",
        "from pyspark.sql.types import DoubleType, StringType, StructType, StructField\n",
        "\n",
        "# Complete the schema\n",
        "\n",
        "### Reload the DataFrame with the new schema, then printSchema to check it\n",
        "# .schema\n",
        "# Load .csv with header, user defined schema and ',' seperators\n",
        "# Show the first 5 rows of data and print the schema\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hw1i5n_HvRv9"
      },
      "source": [
        "Finally, you can define the schema with a DDL (Data Definition Language) string. In this case, the string defines each column name and data type pair, and can be fed into the same `.schema()` method as before. Try this now, and print the schema."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbFn8dcdv0QP"
      },
      "outputs": [],
      "source": [
        "### Load the DataFrame using a DLL string formatted schema\n",
        "DDLSchema = \"longitude double, latitude double, \"\n",
        "\n",
        "### Reload the DataFrame with the new schema, then printSchema to check it\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RX9WLn8awyqA"
      },
      "source": [
        "# Exercise 2\n",
        "\n",
        "To save data stored in a DataFrame, use the `.write` method. you can save your data to a number of formats with PySpark. In addition to saving the data as a new .csv, popular formats include [Parquet files](https://parquet.apache.org/documentation/latest/) and JSON files.\n",
        "\n",
        "Save your DataFrame as a .csv, .parquet, and .JSON file. For the parquet file, set the `.option()` `'compression', 'snappy'`. For the csv file, set the `'header', 'True'`, and `'delimeter', ','`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mr9LrWzFzRn6"
      },
      "outputs": [],
      "source": [
        "### Save the DataFrame as a .csv, .parquet and .JSON\n",
        "# .write, .option, .csv, .parquet, .json\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPNHnwNX3b_9"
      },
      "source": [
        "An alternative way to save the DataFrame is to specify the `.format()` of the file and use the `.save()` method. Repeat the previous oiperations, but this time use the `.format('string')` method, the `.mode('overwrite')` method, and the `.save('/path/to/file') method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhpe1E9h4Y4W"
      },
      "outputs": [],
      "source": [
        "### Use the .save() method to save the DataFrame as a .csv, .parquet and .JSON\n",
        "# .write, .format, .option, .mode, .save\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zbIa7bq5bZ3"
      },
      "source": [
        "# Exercise 3\n",
        "\n",
        "DataFrames can be manipulated using the built-in SQL API. The methods can be used to select columns from the DataFrame, apply filters and masks, sort, or group data, and much more. In this exercise, you will need the following methods:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        ".select() # one or more column names\n",
        ".where() # boolean expression\n",
        ".groupBy() # column name\n",
        ".count()\n",
        ".orderBy() # column name, ascending=True/False\n",
        "```\n",
        "\n",
        "The SQL operations aren't evaluated immediately, and return a new dataframe. By appending `.show()`, you can trigger the calculation and display the new dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24N0vxtn6d7x"
      },
      "outputs": [],
      "source": [
        "### Example: Select the median income and house value columns, sort by income\n",
        "# .select, .orderBy/.sort\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLG7baDY8bHk"
      },
      "outputs": [],
      "source": [
        "### Select the median house age and house value columns and order by total\n",
        "### bedrooms in descending order\n",
        "# .select, .orderBy/.sort\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HL2UnexC9HFb"
      },
      "outputs": [],
      "source": [
        "### Select median income and house value, where ocean proximity is NEAR_BAY\n",
        "# .select, .filter/.where\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2DfwkZ89Ne-"
      },
      "outputs": [],
      "source": [
        "### Count the number of entries where population > 500\n",
        "#Â .filter/.where, .count\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAMcfqMv_Cop"
      },
      "outputs": [],
      "source": [
        "### Group by ocean proximity and count the number of entries in each category\n",
        "# .groupBy, .count\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCoh7eNEXAaI"
      },
      "source": [
        "# Exercise 4\n",
        "\n",
        "The `Column` class is another way to access and manipulate the data within the DataFrame. You can use Columns to form complex expressions, such as:\n",
        "```\n",
        "col('total_bedrooms') / col('total_rooms')\n",
        "col('median_house_value').desc()\n",
        "(col('median_income')*1000).cast('int')\n",
        "```\n",
        "For the following tasks, use Column objects in the DataFrame transformations. Create a new DataFrame for each task, and show the contents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2xAqRybXHac"
      },
      "outputs": [],
      "source": [
        "### Example: Create a DataFrame with only rows where population > 500, include\n",
        "### a column with the number of bedrooms / total number of rooms, and sort by\n",
        "### descending house value\n",
        "# col, .filter/.where, withColumn, .orderBy/.sort, desc\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Column expression to calculate ratio of bedrooms and rooms\n",
        "\n",
        "# Filter by population, add new column, sort DF\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LE71Ydh-Y53d"
      },
      "outputs": [],
      "source": [
        "### Select the population and median house value where\n",
        "### the median house age is < 20, store the result as a new DataFrame\n",
        "# col, .select, .where"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0C1IR2g9Zz07"
      },
      "outputs": [],
      "source": [
        "### Create a new DataFrame where the ocean proximity column has been dropped.\n",
        "# col, .drop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YoZ_pkrZ-Ai"
      },
      "outputs": [],
      "source": [
        "### Create a DataFrame which includes a new column for population per household,\n",
        "### sort by that column, and rename 'ocean_proximity' to 'location'\n",
        "# col, .withColumn, .withColumnRenamed, .sort/.orderBy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G68MJq9DctuR"
      },
      "outputs": [],
      "source": [
        "### Create a DataFrame with the null values in total_bedrooms removed\n",
        "# col, .isNotNull, .where/.filter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qa212n1mdAW_"
      },
      "outputs": [],
      "source": [
        "### Using the collect method to return the listed values of a column, create a\n",
        "### scatter plot in matplotlib with 'longitude' vs 'latitude', coloured by the\n",
        "### 'median_house_value'. Include appropriate axis labels, and a labelled\n",
        "### colour bar.\n",
        "# .select, .collect, .scatter, .colorbar\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "longitude = usersDF.select(col('longitude')).collect()\n",
        "latitude = []\n",
        "house_value = []"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}