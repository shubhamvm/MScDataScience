{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial 9\n",
        "\n",
        "In this tutorial, we will explore some ways to optimise input data pieplines. Begin by running the code below, then follow the instructions in the next section."
      ],
      "metadata": {
        "id": "317koPrlZzoz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqpu8Io7w6GJ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import models, layers, optimizers, losses\n",
        "import PIL\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading in an image dataset, this one includes images of different types of\n",
        "# flowers\n",
        "import pathlib\n",
        "dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
        "data_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url, untar=True)\n",
        "data_dir = pathlib.Path(data_dir)\n",
        "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
        "print(image_count)\n",
        "roses = list(data_dir.glob('roses/*'))\n",
        "PIL.Image.open(str(roses[0]))\n"
      ],
      "metadata": {
        "id": "gkG6KdOe8cYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataset from the file names of the images and make a list of the\n",
        "# class names\n",
        "list_ds = tf.data.Dataset.list_files(str(data_dir/'*/*'))\n",
        "class_names = ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']"
      ],
      "metadata": {
        "id": "mGxAHnSlBC3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First function for processing images and labels. This one opens and decodes\n",
        "# the images, and converts the label from a string into an interger\n",
        "def process_image_label(filename):\n",
        "    parts = tf.strings.split(filename, os.sep)\n",
        "    one_hot = parts[-2] == class_names\n",
        "    label = tf.argmax(one_hot)\n",
        "\n",
        "    image = tf.io.read_file(filename)\n",
        "    image = tf.io.decode_jpeg(image)\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    image = tf.image.resize(image, [128, 128])\n",
        "    image = (image / 255.0)\n",
        "    return image, label"
      ],
      "metadata": {
        "id": "P8geX3nVIiHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# An augmentation function which will manipulate images randomly on each epoch\n",
        "def augment(image, label, seed):\n",
        "    # We are using the \"stateless\" random functions, so we need to generate\n",
        "    # random seeds\n",
        "    new_seed = tf.random.experimental.stateless_split(seed, num=1)[0, :]\n",
        "    image = tf.image.stateless_random_flip_left_right(\n",
        "        image, seed)\n",
        "    image = tf.image.stateless_random_brightness(\n",
        "        image, max_delta=0.5, seed=new_seed)\n",
        "    image = tf.image.stateless_random_hue(\n",
        "        image, 0.1, seed)\n",
        "    image = tf.image.stateless_random_saturation(\n",
        "        image, 0.5, 1.0, seed)\n",
        "\n",
        "    image = tf.clip_by_value(image, 0, 1)\n",
        "    return image, label\n"
      ],
      "metadata": {
        "id": "eGiicsKoBWmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a wrapper function for updating seeds.\n",
        "def f(x, y):\n",
        "  seed = rng.make_seeds(2)[0]\n",
        "  image, label = augment(x, y, seed)\n",
        "  return image, label"
      ],
      "metadata": {
        "id": "aKPIoMFJHpM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a random generator.\n",
        "rng = tf.random.Generator.from_seed(123, alg='philox')"
      ],
      "metadata": {
        "id": "Fv5dcLyjCiUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### YOU'LL NEED TO COPY THIS BLOCK DOWN BELOW AND EDIT IT\n",
        "\n",
        "# Create the basic datasets. Images are split into a training and validation set\n",
        "val_size = int(image_count * 0.2)\n",
        "train_ds = list_ds.skip(val_size)\n",
        "val_ds = list_ds.take(val_size)\n",
        "\n",
        "# Training images are parsed with the processing and augmentation functions\n",
        "train_ds = (\n",
        "    train_ds\n",
        "    .shuffle(1000)\n",
        "    .map(process_image_label)\n",
        "    .map(f)\n",
        "    .batch(32)\n",
        ")\n",
        "\n",
        "# Validation images are only parsed with the processing function\n",
        "val_ds = (\n",
        "    val_ds\n",
        "    .map(process_image_label)\n",
        "    .batch(32)\n",
        ")"
      ],
      "metadata": {
        "id": "YW3bPQ7-JLa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A fairly simple CNN, nothing fancy going on here\n",
        "num_classes = len(class_names)\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=(128,128,3)),\n",
        "    layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(num_classes)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "IhRPezZXSS_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model. We don't care so much about the results, more how long it\n",
        "# takes to process two epochs\n",
        "epochs=2\n",
        "model.fit(\n",
        "  train_ds,\n",
        "  validation_data=val_ds,\n",
        "  epochs=epochs\n",
        ")\n"
      ],
      "metadata": {
        "id": "MmvfsBVySiWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 1\n",
        "\n",
        "Copy the code block that defines the datasets from above. In this exercise, you will be changing the dataset definition to improve the training times. For each experiment, define the new datasets, then run for two epochs and note the time it takes to process both.\n",
        "\n",
        "Start by adjusting the batch sizeâ€”see what happens when you alter the batch size from 16 images up to 256 images, in powers of 2."
      ],
      "metadata": {
        "id": "EAJNDqMwkgRF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "98yw2WJtleZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 2\n",
        "\n",
        "Using the [Tensorflow Data Performance Guide](https://www.tensorflow.org/guide/data_performance#overview) as a reference, make edits to the data pipeline one change at a time, and see how it affects the training times. Consider how the order of the dataset transformations might affect the training process.\n",
        "\n",
        "Your final pipeline could make use of the following methods (not necessarily in this order):\n",
        "```\n",
        ".prefetch\n",
        ".map\n",
        ".cache\n",
        "```\n",
        "\n",
        "You can make use of parallel processing in the data pipeline by using the argument `num_parallel_calls=tf.data.AUTOTUNE` in most of the dataset methods."
      ],
      "metadata": {
        "id": "f3NKhhMclfMX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dfoP39oGnd3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 3\n",
        "\n",
        "Use the TensorFlow profiler to explore the performance of the model on the GPU."
      ],
      "metadata": {
        "id": "OqyCD22nnvft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U tensorboard_plugin_profile"
      ],
      "metadata": {
        "id": "uMBVBMKOn4Kp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "# Create a TensorBoard callback\n",
        "logs = \"logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "# You'll need to edit the profile_batch here so that it profiles 10 batches\n",
        "# in the second epoch of your training\n",
        "tboard_callback = tf.keras.callbacks.TensorBoard(log_dir = logs,\n",
        "                                                 histogram_freq = 1,\n",
        "                                                 profile_batch = (50,60))\n"
      ],
      "metadata": {
        "id": "gyQrqb3Cn_OG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_ds,\n",
        "          epochs=2,\n",
        "          validation_data=val_ds,\n",
        "          callbacks = [tboard_callback])"
      ],
      "metadata": {
        "id": "fG9Po4nAoTQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the TensorBoard notebook extension.\n",
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "aNdwYrezoatt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Launch TensorBoard and navigate to the Profile tab to view performance profile\n",
        "%tensorboard --logdir=logs"
      ],
      "metadata": {
        "id": "yHCLbdUxoc0t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}