{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViQdeQ5m1E2Z"
      },
      "source": [
        "# ADS2 - Tutorial 3 - Aggregations\n",
        "\n",
        "Learning Outcomes:\n",
        "\n",
        "1.   Use Aggregation functions to explore the properties of a DataFrame\n",
        "2.   Use GroupedData to perform multiple aggregations at once, over specific subsets of data\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "QR84bWkSrmKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lb-Z7ZM8O3s"
      },
      "outputs": [],
      "source": [
        "# Apache Spark uses Java, so first we must install that\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Unpack Spark from google drive\n",
        "!tar xzf /content/drive/MyDrive/spark-3.3.0-bin-hadoop3.tgz\n",
        "\n",
        "# Set up environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"spark-3.3.0-bin-hadoop3\"\n",
        "\n",
        "# Install findspark, which helps python locate the psyspark module files\n",
        "!pip install -q findspark\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Finally, we initialse a \"SparkSession\", which handles the computations\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()\n",
        "\n",
        "from pyspark.sql import functions as F"
      ],
      "metadata": {
        "id": "IhZt_RK1P7MP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N61alIq17Q4i"
      },
      "source": [
        "# Exercise 1\n",
        "\n",
        "Upload and read the all-weeks-countries.csv file from the canvas page into a DataFrame. The dataset is described [here](https://www.kaggle.com/datasets/dhruvildave/netflix-top-10-tv-shows-and-films)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Read in the .csv data, ensure the schema is appropriate\n",
        "\n",
        "CsvPath = '/content/all-weeks-countries.csv'\n",
        "\n",
        "# Load .csv with header, ',' seperators and inferred schema\n",
        "NetflixDF =\n",
        "\n",
        "# Print Schema to check\n"
      ],
      "metadata": {
        "id": "IBhay169pd7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Split the dataframe into two separate tables, one for films and one for TV\n",
        "### Display the two tables\n",
        "# .filter\n",
        "\n"
      ],
      "metadata": {
        "id": "TqBNemfhteoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 2\n",
        "\n",
        "Aggregate [functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#functions) can be accessed through `pyspark.sql.functions`, this has been imported as `F` for ease of use. To perform a simple aggregation, you can call the function on a column name, then pass it to the `.select` method."
      ],
      "metadata": {
        "id": "z7Q8QZEcwkCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### EXAMPLE: Find the TV show with the most weeks in the top 10\n",
        "\n",
        "# Find max weeks in top 10, select that column\n",
        "tvDF.select(F.max('cumulative_weeks_in_top_10')).show()\n",
        "\n",
        "# To access the number in the DataFrame, use .first()[0]\n",
        "tvDF.select(F.max('cumulative_weeks_in_top_10')).first()[0]\n"
      ],
      "metadata": {
        "id": "OnZichi48M-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Find the mean number of weeks in the top 10 for all films in the dataset\n",
        "### Then use the .filter method to find the mean rating for\n",
        "### films in the UK and Germany\n",
        "# .select, .mean, .filter\n",
        "\n"
      ],
      "metadata": {
        "id": "VMADFmCSt9kH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Use the .count_distinct aggregate function to find the number of unique\n",
        "### films and TV shows and TV seasons in the dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "bGT0EM65yVgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### You aren't limited to selecting a single aggregate column\n",
        "### Using the .count_distinct function, find the\n",
        "### number of unique TV shows and the number of unique seasons\n"
      ],
      "metadata": {
        "id": "pvovOqvzyfFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Use the .collect_set function to get a list of all the\n",
        "### unique films in the dataset, in alphabetical order\n",
        "# .select, .sort_array, .collect_set\n"
      ],
      "metadata": {
        "id": "7NLCaAGVySPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 3\n",
        "\n",
        "The `.groupBy()` method produces a GroupedData object, which can in turn be used to perform aggregations. You can even group over multiple columns."
      ],
      "metadata": {
        "id": "qem_nGri0wwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### EXAMPLE: Find the mean and max weeks in top 10 per country\n",
        "\n",
        "# group by year, feed aggregations into .agg, use .alias to rename new columns\n",
        "tvDF.groupBy('country_name')\\\n",
        "       .agg(F.mean('cumulative_weeks_in_top_10').alias('mean_weeks_in_top_10'),\n",
        "            F.max('cumulative_weeks_in_top_10').alias('max_weeks_in_top_10'))\\\n",
        "       .show()"
      ],
      "metadata": {
        "id": "Xnks2pAY-GKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Use the \"where\" method to create a new dataframe containing the data for\n",
        "### the show Stranger Things in the Uniter Kingdom. Call this dataframe STDF.\n",
        "# .where()\n",
        "\n",
        "\n",
        "### Using \"groupBy\" method and \"F.count_distinct\" function, find the total number of weeks\n",
        "### Stranger Things spent in the top 10 of the UK, across all seasons. Show the\n",
        "### result.\n",
        "# .groupBy(), .agg(), F.count_distinct()\n",
        "\n"
      ],
      "metadata": {
        "id": "DistzuzmS56e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Produce a dataframe containing the top 25 seasons by number of weeks in the\n",
        "### top 10 of the United Kingdom, sorted by number of weeks.\n",
        "# .where(), .groupBy(), .max(), .sort(), .limit()\n",
        "\n"
      ],
      "metadata": {
        "id": "X6-AU71wzq3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The column below finds the number of weeks a show spent at thge number 1 spot\n",
        "# in the Top 10.\n",
        "\n",
        "weeks_at_1 = F.when(F.min('weekly_rank')==1,\n",
        "                    F.count_distinct('week'))\\\n",
        "                    .otherwise(0)\\\n",
        "                    .alias('weeks_at_1')\n",
        "\n",
        "### Group by country name and show title, and use the .agg method and the new\n",
        "### column to find the number of weeks each film spent in the top spot for each\n",
        "### country.\n",
        "# .groupBy(), .agg(), .sort()\n",
        "\n",
        "### Produce a dataframe grouped by country name that contains the show title and\n",
        "### number of weeks at the number 1 spot of the top performing film in each\n",
        "### country.\n",
        "# .groupBy(), .agg(), F.first()\n",
        "\n"
      ],
      "metadata": {
        "id": "estnrj_sTSQv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}